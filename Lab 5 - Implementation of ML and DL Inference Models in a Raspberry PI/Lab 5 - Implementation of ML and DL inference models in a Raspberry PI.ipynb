{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a38ae7f-644c-411d-aa3e-8b2f4de9bd09",
   "metadata": {},
   "source": [
    "<table style=\"background-color:#F5F5F5;\" width=\"100%\">\n",
    "<tr><td style=\"background-color:#F5F5F5;\"><img src=\"logo.png\" width=\"300\" align='right'/></td></tr>     <tr><td>\n",
    "            <h1><center>Aplicações Avançadas de Instrumentação Biomédica (AAIB)</center></h1>\n",
    "            <h3><center>1st Semester - 2025/2026</center></h3>\n",
    "            <h4><center>Universidade Nova de Lisboa - Faculdade de Ciências e Tecnologia</center></h4>\n",
    "</td></tr>\n",
    "    <tr><td><h1><center>Lab 5 - Implementation of ML and DL Inference Models in a Raspberry PI</center></h1></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b98bce-e99d-4329-a490-be0c065c2543",
   "metadata": {},
   "source": [
    "## Goals:\n",
    "\n",
    "* To run, in the RPI, the classification programs develop in Lab2 and Lab3.\n",
    "* To train new audio and imagem models, from new datasets and do classification in the RPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1cf220-a5b5-4af4-8779-4776c0cca9ef",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    "\n",
    "None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7e5fd-9239-4b48-8f2d-37a797af687c",
   "metadata": {},
   "source": [
    "## Goal 1.1 - Running in the RPi Classification Model from Lab2 (audio)\n",
    "\n",
    "* Create a new folder in the RPI for this exercise\n",
    "* Copy to the audio files from Lab2 to the new RPI folder\n",
    "* Copy the python (*.py) inference program of Lab2 goal 2.4 to the RPI\n",
    "  * If your inference program is still in notebook, convert it for a python program, using spyder or equivalent\n",
    "* Copy to the RPI the ML model. The default name is 'model_audio.pkcls', but it is possible that you are using a diferent name\n",
    "* In the command prompt of the RPI execute the inference program, by doing: *python \\<name of the program\\>*. Confirm that the classification is correct\n",
    "\n",
    "**Note:** It is possible your python program do not execute at first time. Try to understand the errors and debug the program\n",
    "\n",
    "A possible solution for this exercise can be found in goal_1_1_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef9700-9fb5-45e2-ad42-fc0950c11279",
   "metadata": {},
   "source": [
    "## Goal 1.2 - Running in the RPi Classification Model from Lab2 (image)\n",
    "\n",
    "* Create a new folder in the RPI for this exercise\n",
    "* Copy to the test images files from Lab2 to the new RPI folder\n",
    "* Copy the python (*.py) inference program of Lab3 goal 2.1 to the RPI\n",
    "  * If your inference program is still in notebook, convert it for a python program, using spyder or equivalent\n",
    "* Copy to the RPI the FNN model. The default name is 'model_FNN.pth', but it is possible that you are using a diferent name\n",
    "* In the command prompt of the RPI execute the inference program, by doing: *python \\<name of the program\\>*. Confirm that the classification is correct\n",
    "\n",
    "**Note:** It is possible that the python program do not execute at first time. Try to understand the errors and debug the program\n",
    "\n",
    "A possible solution for this exercise can be found in goal_1_2_FNN_solution.py. You will find also solutions for CNN and ResNet Networks in: goal_1_2_CNN_solution.py and goal_1_2_RESNET_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ff931-490f-4236-a92a-932da8ba3174",
   "metadata": {},
   "source": [
    "## Goal 2.1 - New Train and Classification with Signal Data \n",
    "\n",
    "If you aiming for a project using signal, as in inertial or sound, you should complete this exercise. For the process of training your ML classifier you should use a methodologie similar to lab2.\n",
    "\n",
    "The goal of this exercise is to identify audios where a single number, between 0 and 9, is played. So it is necessary to build a classifier with 10 classes. \n",
    "\n",
    "* In the AudioDataset folder you will find two sub-folders: train and validation. You should use the first one to train your model and the second one to validate it.\n",
    "* You should complete the following steps:\n",
    "  * With jupyter notebook, open all the train audio files, extract its feature and create a csv file with them organised by classes.\n",
    "  * In Orange read the csv file, create the best ML model possible and save it.\n",
    "  * In spyder (or equivalent) create an classification program, that uses the ML model previously created and the validation audio files. Analyse the performace of you model.\n",
    "* Try the inference program in the RPI\n",
    "  * Copy to the RPI the validation audio files from the AudioDataSet\n",
    "  * Copy to the RPI the inference program\n",
    "  * Test the classification program and analyse its performance.\n",
    "\n",
    "A possible solution for this exercise can be found in goal_2_1_solution.py. (solution available from 20/10/25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf85bb-ef01-45a3-bd47-aa883cfd6812",
   "metadata": {},
   "source": [
    "## Goal 2.2 - New Train and Classification with Image Data \n",
    "\n",
    "If you aiming for a project using image, you should complete this exercise. For the process of training your ML classifier you should use a methodologie similar to lab3. It is advisable to start with a simple FNN network.\n",
    "\n",
    "The goal of this exercise is to identify images where a single number, between 0 and 9, is handwritten. So it is necessary to build a classifier with 10 classes.\n",
    "\n",
    "* In the ImgDataset folder you will find two sub-folders: train and validation. You should use the first one to train your model and the second one to validate it.\n",
    "* You should complete the following steps:\n",
    "  * With jupyter notebook, use all the train image files to train a model. It is advisable for you to start with a FNN network\n",
    "  * When you consider that the accuracy is acceptable, save the model. It is possible that some parameters of the training needs to be adjusted for a better performance\n",
    "  * In spyder (or equivalent) create an classification program, that uses the FNN model previously created and the validation image files. Analyse the performace of you model.\n",
    "* Try the inference program in the RPI\n",
    "  * Copy to the RPI the validation image files from the ImgDataSet\n",
    "  * Copy to the RPI the inference program\n",
    "  * Test the classification program and analyse its performance.\n",
    "* You can try to run other networks, as the CNN or the ResNet. Than is achivable with minor modification in your programs\n",
    "\n",
    "A possible solution for this exercise can be found in goal_2_2_FNN_solution.py and  goal_2_2_ResNet_solution.py. (solution available from 20/10/25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3905b1e-3c26-48e0-bf1a-f56ab253707a",
   "metadata": {},
   "source": [
    "## Goal 2.2.1 - Data Augmentation (optional)\n",
    "\n",
    "In DL models involving images, it is often necessary to do data augmentation in order to improve the model generalization. Examples of that, is the classifications of imagens where is not guaranteed, that are all in some orientation and/or position. In this situation data augmentation with rotation and/or translation is advisable.\n",
    "\n",
    "Data augmentation, when requested, it is managed by pytorch that applies the transformation to the existing data set, in order to generate more images. This new images does not introduce more information to the model, only improves generalization of the existing information. It is also not guaranteed that it will improve the accuracy of the model, as in a situation where the imagens does not rotate, doing rotation data augmentation will reduce the model performance.\n",
    "\n",
    "In the solution folder, you can find goal_2_2_1_FNN_augmented_solution.ipynb and goal_2_2_1_ResNet_augmented_solution as examples where data augmentation was performed. (solution available from 20/10/25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2dbea3-1fdc-4e31-ae3f-5d7a2fb5206b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
